<!DOCTYPE html>
<html lang='en'>
  <head>
  <meta charset='utf-8'>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>NeurIPS 2018 - AI for Prosthetics Challenge | Your awesome title</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="NeurIPS 2018 - AI for Prosthetics Challenge" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My team participated in NeurIPS 2018 - AI for Prosthetics Challenge and took 17th place out of 401 teams. We were able to get valuable practical experiences with reinforcement learning and biomechanics domains." />
<meta property="og:description" content="My team participated in NeurIPS 2018 - AI for Prosthetics Challenge and took 17th place out of 401 teams. We were able to get valuable practical experiences with reinforcement learning and biomechanics domains." />
<link rel="canonical" href="http://localhost:4000/portfolio/medipixel/nips_2018_-_AI_for_prosthetics_challenge" />
<meta property="og:url" content="http://localhost:4000/portfolio/medipixel/nips_2018_-_AI_for_prosthetics_challenge" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-02T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="NeurIPS 2018 - AI for Prosthetics Challenge" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-04-02T00:00:00+09:00","datePublished":"2019-04-02T00:00:00+09:00","description":"My team participated in NeurIPS 2018 - AI for Prosthetics Challenge and took 17th place out of 401 teams. We were able to get valuable practical experiences with reinforcement learning and biomechanics domains.","headline":"NeurIPS 2018 - AI for Prosthetics Challenge","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/portfolio/medipixel/nips_2018_-_AI_for_prosthetics_challenge"},"url":"http://localhost:4000/portfolio/medipixel/nips_2018_-_AI_for_prosthetics_challenge"}</script>
<!-- End Jekyll SEO tag -->

  <link rel='stylesheet' href='/assets/main.css'>
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" />
  

</head>

  <body>
    <div class='layout'>
      <header class='header'>
  <nav class='header__nav'>
    
      
        <a class='header__nav__item' href='/'>Hyungkyu Kim</a>
      
    
      
        <a class='header__nav__item--selected' href='/portfolio'>Portfolio</a>
      
    
  </nav>
  <hr width=100% border-color=#EBEBEB size=1>
</header>
      <main class='main'>
        <article class='post'>
  <div class="posts__navigation">
    <a href="/portfolio"> Portfolio </a> &nbsp; > &nbsp; <a href="/portfolio#Medipixel"> Medipixel </a> &nbsp; > &nbsp; NeurIPS 2018 - AI for Prosthetics Challenge
  </div>
  <br/>
  <div>
  
    
  
    
      <img src="/images/medipixel/logo_medipixel.png" width="100px" style="margin-bottom:35px">
      
  </div>
  <h1>NeurIPS 2018 - AI for Prosthetics Challenge</h1>
  <p>My team participated in <a href="https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge">NeurIPS 2018 - AI for Prosthetics Challenge</a> and took 17th place out of 401 teams. 
We were able to get valuable <a href="https://medipixel.github.io/authors/hyungkyu-kim/">practical experiences</a> with reinforcement learning and biomechanics domains.</p>

<!--break-->

<h4 id="my-role">My Role</h4>
<p>System main engineer</p>
<ul>
  <li>Implement distributed Reinforcement Learning system(<a href="https://rise.cs.berkeley.edu/projects/ray/">RAY</a>) on Cloud system(<a href="https://azure.microsoft.com/">Azure</a>)</li>
  <li>Implement RL-agent in <a href="https://rise.cs.berkeley.edu/projects/ray/">RAY</a></li>
</ul>

<hr />

<h4 id="project-detail">Project Detail</h4>

<figure>
  <img src="/images/medipixel/nips_final.png" width="80%" />
  <figcaption></figcaption>
</figure>

<p>AI for Prosthetics Challenge was a official competition in <a href="https://nips.cc/Conferences/2018/CompetitionTrack">NeurIPS 2018 Competition Track</a>. 
It used <a href="http://opensim.stanford.edu/">Opensim</a> as a environment. 
The topic of this competition was to control neuromusculoskeletal model of person with prosthetics in the right leg for moving forward at the given velocity.</p>

<p>We tried many approaches to save time and overcome shortage of resources. 
First, we researched <a href="http://osim-rl.stanford.edu/docs/nips2017/solutions/">previous competition</a> and gathered useful information from them. 
Second, since my team was just two members, we divided the work up between each member cleary. 
The areas that my team focused on were RL method study, Opensim simulator study and imitation learning study.</p>

<h5 id="achievements">Achievements</h5>
<p>Since it was the first massive and practical RL project that my team undertook, we had numerous failures. 
But, in such attempts <strong>it was possible to obtain a skill-set to handle practical RL problem.</strong></p>
<ul>
  <li>Handle complex environment(hundreds state values, 20  action values)</li>
  <li>Handle sparse reward problem in RL</li>
  <li>Build distributed Reinforcement Learning system(<a href="https://rise.cs.berkeley.edu/projects/ray/">RAY</a>) on Cloud system(<a href="https://azure.microsoft.com/">Azure</a>)</li>
  <li>Utilize imitation learning method</li>
</ul>

<h5 id="environment-analysis">Environment Analysis</h5>
<p>To begin with this competition, I had to analyse unfamiliar environment.
As opensim simulation and walking/running gait sequence were far from main task that I had handled before, I put efforts to understand it.</p>

<h6 id="opensim-environment">Opensim Environment</h6>
<p>First, I started to verify RL environment space.
In <a href="https://www.endtoend.ai/blog/ai-for-prosthetics-2/">action space</a>, it was a complex space controlling each muscle one by one compared to other general control methods utilizing joints velocity and angle.
Also <a href="http://osim-rl.stanford.edu/docs/nips2018/observation/">observation space</a> was relatively complicated, because it had hundreds of data spectated from musculoskeletal.</p>

<figure>
  <img src="https://raw.githubusercontent.com/stanfordnmbl/osim-rl/1679344e509e29bdcc2ee368ddf83e868d93bf61/demo/random.gif" width="50%" />
  <figcaption>from (http://osim-rl.stanford.edu/) </figcaption>
</figure>

<h6 id="opensim-tools">Opensim Tools</h6>
<p>To achieve high score, I had to use <a href="https://simtk.org/projects/nmbl_running">experimental datasets</a> for walking/running gait sequence in Opensim community. 
I tried to figure out the pattern of joint movements in gait cycle at the provided dataset.
There were several Opensim applications to solve biomechanical problems through forward and Inverse problem.</p>
<figure>
  <img src="https://medipixel.github.io/img/opensim/opensim_01.png" width="70%" />
  <figcaption>OpenSim: Simulating musculoskeletal dynamics and neuromuscular control to study human and animal movement</figcaption>
</figure>

<p>Furthermore, it was necessary to analyze tools in code level, since built-in application had a limitation and I needed to modify experiment dataset in various ways.
After comprehension, I made it possible to customize tools based on project needs.</p>
<figure>
  <img src="https://medipixel.github.io/img/imitation/reward_edit_motion.PNG" width="70%" />
</figure>

<h5 id="utilization-of-distributed-rl">Utilization of Distributed RL</h5>
<p>It was one of the most critical problems that interaction with simulator environment needed huge time while training. 
To improve training performance, my team used distributed Reinforcement Learning system.
It was implemented via <a href="https://rise.cs.berkeley.edu/projects/ray/">RAY</a> on <a href="https://azure.microsoft.com/">Azure</a> cloud server. 
We operated four cloud servers, and each server instance had 72 high-speed cores.
And all of server was clustered via Ray.
In result, It was possible to achieve much faster convergence of RL training results.</p>
<figure>
  <img src="/images/medipixel/nips_system.png" width="70%" />
</figure>

<h5 id="imitation-learning">Imitation Learning</h5>
<p>Sparse reward was a big trouble. 
In this competition, There was no clue for reward of RL agent except for the running speed. 
I tried to solve this problem with reward shaping and imitation learning method.</p>

<h6 id="demonstration">Demonstration</h6>
<p>First of all, I created running demonstration from provided dataset because RL agent needed to refer to expertâ€™s trajectories.
Having limited period of gait cycle in Experiment dataset, I needed to insert additional kinematic motion data. 
So, I analysed pattern of gait from other referenced experiments(eg. <a href="https://www.ncbi.nlm.nih.gov/pubmed/18822415">Muscle contributions to support and progression over a range</a>) and made omitted data.</p>

<table>
  <tbody>
    <tr>
      <td><img src="https://medipixel.github.io/img/opensim/opensim_30002_ik.gif" /></td>
      <td><img src="https://medipixel.github.io/img/opensim/opensim_run_demo0.gif" /></td>
    </tr>
  </tbody>
</table>

<h6 id="behavioral-cloning-from-observation">Behavioral Cloning from Observation</h6>
<p>It failed to obtain actions from Opensim application with provided kinematic data. 
I needed to use a new approach that utilized only kinematic data for training. 
<a href="https://arxiv.org/abs/1805.01954">Behavioral Cloning from Observation</a>(BCO) was one of the best options because Ray had BC agent and it was possible to implement BCO by exploiting built-in BC agent.
BCO was a model-based RL method that used model to infer actions from kinematics.
Therefore, I implemented <a href="https://github.com/HyungKyu-Kim/ray/tree/master/python/ray/rllib/agents/bco">BCO agent in Ray</a>.</p>

<h6 id="deepmimic">Deepmimic</h6>
<p>Unfortunately, BCO method did not take notable score. 
Complexity of Opensim environment was too high to obtain meaningful state transition data from not-well-trained agent interactions. 
So I used state of the art method of imitation learning called <a href="https://arxiv.org/abs/1804.02717">Deepmimic</a>.
I implemented <a href="">reward function of Deepmimic in Opensim environment</a>.</p>
<figure>
  <img src="http://bair.berkeley.edu/static/blog/stuntman/teaser.gif" />
  <figcaption>from https://bair.berkeley.edu/blog/2018/04/10/virtual-stuntman/</figcaption>
</figure>

<h5 id="experiment">Experiment</h5>
<p>As deepmimic had a huge parameter set, I had to have many parameter-tuning tasks to improve score until project deadline.
Examples of early phase training by various parameters are like below.</p>

<figure>
  <img src="/images/medipixel/nips_graph.PNG" />
</figure>

<h5 id="result">Result</h5>
<p>Unlike my expectation, even we used Deepmimic method, agent could not imitate movement of demonstrations perfectly.
But Deepmimic was so helpful that agent achieved higher score than heuristic reward shaping method.
The final results were like below</p>

<table>
  <tbody>
    <tr>
      <td><img src="https://medipixel.github.io/img/imitation/reward_ars_demo.gif" /></td>
      <td><img src="https://medipixel.github.io/img/imitation/reward_final.gif" /></td>
    </tr>
  </tbody>
</table>

</article>

      </main>
      <footer class='footer'>
  <ul class='footer__list'>
    

    
      <li class='footer__item'>
        <a href='https://github.com/jekyll' title='github'>
          <svg class='icon'><use href='/assets/icons/github.svg#github' xlink:href='/assets/icons/github.svg#github'></use>
</svg>

        </a>
      </li>
    

    

    
      <li class='footer__item'>
        <a href='https://twitter.com/jekyllrb' title='twitter'>
          <svg class='icon'><use href='/assets/icons/twitter.svg#twitter' xlink:href='/assets/icons/twitter.svg#twitter'></use>
</svg>

        </a>
      </li>
    

    

    
  </ul>
</footer>

    </div>
  </body>
</html>
