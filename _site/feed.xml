<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-06-19T03:01:40+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hyungkyu Kim</title><subtitle>Hyungkyu Kim's blog</subtitle><author><name>Hyungkyu Kim</name></author><entry><title type="html">Floor flatness measuring device</title><link href="http://localhost:4000/portfolio/private/flatness" rel="alternate" type="text/html" title="Floor flatness measuring device" /><published>2019-04-28T00:00:00+09:00</published><updated>2019-04-28T00:00:00+09:00</updated><id>http://localhost:4000/portfolio/private/Floor%20flatness%20measuring%20device</id><content type="html" xml:base="http://localhost:4000/portfolio/private/flatness">&lt;p&gt;To solve floor flatness problem in construction site, I have been making device to check flatness level.
It needs to converge technologies including LIDAR and SLAM.
This is an ongoing project.&lt;/p&gt;

&lt;!--break--&gt;

&lt;h4 id=&quot;my-role&quot;&gt;My Role&lt;/h4&gt;
&lt;p&gt;Main developer&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Analyze requirements&lt;/li&gt;
  &lt;li&gt;Design device concept&lt;/li&gt;
  &lt;li&gt;Design system architecture&lt;/li&gt;
  &lt;li&gt;Implement system&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;project-detail&quot;&gt;Project Detail&lt;/h4&gt;

&lt;p&gt;In many construction site, floor leveling is hard to handle perfectly.
It is caused by problems including wrong concrete pouring and uneven base ground.
Especially, this uneven floor makes issues such that floor coating becomes poor looking shapes and special facilities where strict environment is required (e.g. distribution center, precision manufacture) cannot have proper ground state to install equipment.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/private/flat_comparison.PNG&quot; width=&quot;90%&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Many construction companies try to fix this problem by various methods.
One of general solutions is to use grinding machine.
Running grinding machine, operator needs to have sufficient experience to determine how much grinding should be done in order to make the ground even.&lt;/p&gt;

&lt;p&gt;In this process, there are two &lt;strong&gt;drawbacks&lt;/strong&gt; that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Success of grinding depends on personal skill based on eye measurement&lt;/li&gt;
  &lt;li&gt;There is no quantified measurement system on how much an operator levels the ground off&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, if there is a way to &lt;strong&gt;support flatness measurement task through quantifying atypical image data and visualizing data&lt;/strong&gt;, work efficiency and reliability can be increased.
I made a team with my friend and undertook to solve this problem.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/private/flat_grinding.PNG&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;planning&quot;&gt;Planning&lt;/h5&gt;

&lt;p&gt;This project is just first stage.
Start from this project, final purpose that we want to achieve is to improve all procedure of floor leveling. 
Because it is hard to improve all procedure of floor leveling at once, we established three big stages.
Briefly, we approach the goal by below methods&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Quantify atypical data&lt;/li&gt;
  &lt;li&gt;Gather data&lt;/li&gt;
  &lt;li&gt;Evaluate tasks&lt;/li&gt;
  &lt;li&gt;Improve equipment&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;1-measurement-and-visualization&quot;&gt;1. Measurement and Visualization&lt;/h6&gt;
&lt;p&gt;First of all, we need to acquire precise spatio map. 
Only if we obtain clear data, it is possible to progress rest of procedure based on quantified indicator.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/private/flat_map.png&quot; width=&quot;50%&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;2-gathering-field-data&quot;&gt;2. Gathering Field Data&lt;/h6&gt;
&lt;p&gt;If it is possible to provide clear spatio map data, gathering meaningful data from field is feasible.
First, we can calculate score of flatness from measuring data via several evaluating standard of floor flatness.
Second, we can evaluate result of field workers by difference of measuring map data between before task and after task.&lt;/p&gt;

&lt;p&gt;Also, we are going to collect additional data including path of grinding machine in field, material of ground and information of workers.&lt;/p&gt;

&lt;p&gt;Analyzing these data, we decide what is the best task that was done by worker and way how to operate for grinding. 
Furthermore, through best method, it is possible to provide driving guidelines of grinding machine for field workers.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/private/flat_data.png&quot; width=&quot;90%&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;3-improvement-of-grinding-machine&quot;&gt;3. Improvement of Grinding Machine&lt;/h6&gt;
&lt;p&gt;From accumulated data, we expect to find improvable point of grinding machine because present machine has a shortage that it is hard to work in detailed part.
This process includes designing new machine and numerous experiment.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/private/flat_grinder.png&quot; width=&quot;50%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;requirements&quot;&gt;Requirements&lt;/h5&gt;

&lt;p&gt;At the beginning of project, my team investigated requirements from field workers at first.
To acquire sufficient information, we interviewed in many ways.&lt;/p&gt;

&lt;h6 id=&quot;how-accurate-should-a-sensor-be&quot;&gt;How accurate should a sensor be?&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;p&gt;There is no exact numerical criterion. 
In the present process, since most workers check flatness based on eye measurement, it might vary by people.
But, I think that it is necessary for the sensor to be accurate enough to determine whether it would be flat enough to install precision equipment.
It might be about 3mm.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;☛ Reqirements:&lt;/strong&gt; Sensor should enable to check under 3mm.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;how-long-does-it-takes-to-measure-flatness&quot;&gt;How long does it takes to measure flatness?&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;p&gt;Currently, we do not spend extra flatness measurement time.
But as correct ground information map is important, if it is needed extra process to measure and map ground information, we can put additional time and human resources.
In this regard, we request to minimize measurement time as possible because there are other processes in whole procedure.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;☛ Reqirements:&lt;/strong&gt; Measurement time should be minimized as possible.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h6 id=&quot;how-much-budget-to-make-device-proper-is&quot;&gt;How much budget to make device proper is?&lt;/h6&gt;
&lt;blockquote&gt;
  &lt;p&gt;Average budget of procedure is about 0000 and assigned budget on usage of equipment is about 0000.
So, we wish the price of measurement device about 0000.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;☛ Reqirements:&lt;/strong&gt;  Device development cost should be under 0000.&lt;/p&gt;

&lt;h5 id=&quot;research&quot;&gt;Research&lt;/h5&gt;

&lt;p&gt;There are many sensors used for obtaining space information.
Since most of them have a positive correlation between price and accuracy, we need to find a compromise.
As task sites can be both inside and outside, there is a huge possibility that can be affected by variance of illuminate condition. 
Therefore, photogrammetry was excluded in selection and we decided to start from &lt;a href=&quot;https://en.wikipedia.org/wiki/Lidar&quot;&gt;Light Detection And Ranging&lt;/a&gt;(LIDAR) sensor because it has reasonable price and accuracy.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/private/flat_LiDAR.jpeg&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Furthermore, to get spatial map, it is necessary to choose a method to build map by data from sensors.
Thus, we study about &lt;a href=&quot;https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping&quot;&gt;Simultaneous Localization And Mapping&lt;/a&gt;(SLAM).&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/private/flat_SLAM.png&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;</content><author><name>Hyungkyu Kim</name></author><summary type="html">To solve floor flatness problem in construction site, I have been making device to check flatness level. It needs to converge technologies including LIDAR and SLAM. This is an ongoing project.</summary></entry><entry><title type="html">Guidewire Navigation based on Reinforcement Learning</title><link href="http://localhost:4000/portfolio/medipixel/guidewire_navigation_based_on_reinforcement_learning" rel="alternate" type="text/html" title="Guidewire Navigation based on Reinforcement Learning" /><published>2019-04-14T00:00:00+09:00</published><updated>2019-04-14T00:00:00+09:00</updated><id>http://localhost:4000/portfolio/medipixel/Guidewire%20Navigation%20based%20on%20Reinforcement%20Learning</id><content type="html" xml:base="http://localhost:4000/portfolio/medipixel/guidewire_navigation_based_on_reinforcement_learning">&lt;p&gt;The system places stent to the target lesion in the coronary artery via controlling manipulator made by &lt;a href=&quot;http://eng.amc.seoul.kr/gb/lang/main.do&quot;&gt;Asan Medical Center&lt;/a&gt;. 
It is the world first autonomous &lt;a href=&quot;https://en.wikipedia.org/wiki/Percutaneous_coronary_intervention&quot;&gt;PCI&lt;/a&gt; robot system. 
My team has developed reinforcement learning-based control software. 
This project was awarded first place at &lt;a href=&quot;https://jlabs.jnjinnovation.com/quickfire-challenges/seoul-innovation-quickfire-challenge-robotics-digital-surgery&quot;&gt;Johnson &amp;amp; Johnson QuickFire Challenge&lt;/a&gt;.&lt;/p&gt;

&lt;!--break--&gt;

&lt;h4 id=&quot;my-role&quot;&gt;My Role&lt;/h4&gt;
&lt;p&gt;Development Team Leader&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Design overall system architecture&lt;/li&gt;
  &lt;li&gt;Design data flow in system&lt;/li&gt;
  &lt;li&gt;Design an experiment process&lt;/li&gt;
  &lt;li&gt;Establish experiment environment&lt;/li&gt;
  &lt;li&gt;Implement device communication(manipulator, camera) module&lt;/li&gt;
  &lt;li&gt;Implement reinforcement learning environment module&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;project-detail&quot;&gt;Project Detail&lt;/h4&gt;

&lt;!-- Feel free to change the width and height to your desired video size. --&gt;

&lt;div class=&quot;embed-container&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/rTzwVTdZ5Ic&quot; width=&quot;700&quot; height=&quot;480&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
Cardiovascular disease takes up a large portion of annual death around the world. 
&lt;a href=&quot;https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death&quot;&gt;WHO announcement&lt;/a&gt; shows that ischaemic heart disease is the world’s biggest killer in 2016 and one of the most general treatments for ischaemic heart disease is &lt;a href=&quot;https://en.wikipedia.org/wiki/Percutaneous_coronary_intervention&quot;&gt;Percutaneous Coronary Intervention&lt;/a&gt;. 
But &lt;strong&gt;traditional PCI has several shortages&lt;/strong&gt; as follows.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Huge gap of proficiency and difficult transfer of advanced skill&lt;/li&gt;
  &lt;li&gt;Exposure to harmful radiation generated from angiography&lt;/li&gt;
  &lt;li&gt;Long time required(about 1 hour)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To overcome these challenges, Medipixel has been collaborating with &lt;a href=&quot;http://eng.amc.seoul.kr/gb/lang/main.do&quot;&gt;Asan Medical Center&lt;/a&gt; to develop &lt;strong&gt;the world first autonomous PCI robot system&lt;/strong&gt;. 
This system autonomously navigates guide wire to the target lesion.
It means that this system plays a role to transfer atypical procedure requiring personal skill and experience to typical procedure.
The &lt;strong&gt;“Standardization of Procedure”&lt;/strong&gt; process has advantages such as&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Reduce skill gap between an experienced doctor and a novice doctor&lt;/li&gt;
  &lt;li&gt;Reduce exposing time from radiation&lt;/li&gt;
  &lt;li&gt;Enable workers to do other tasks&lt;/li&gt;
  &lt;li&gt;Reduce time&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_animal_ex.png&quot; /&gt;
  &lt;figcaption&gt;Animal test observation - COPYRIGHT© 2017 Medipixel. All Rights Reserved.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;challenges&quot;&gt;Challenges&lt;/h5&gt;
&lt;p&gt;For successful PCI procedure, it is necessary to quantify abstract and complex factors, such as doctor’s experience and medical knowledge, as proper numerical values in system. 
Performing this task, we considered the following issues&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;image&quot;&gt;
        &lt;img src=&quot;/images/medipixel/profile_biorobot_medicine.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;text&quot;&gt;
        &lt;h6&gt;Safety&lt;/h6&gt;
        &lt;br /&gt;
        &lt;ul&gt;
            &lt;li&gt;
                There is possibility that even an unexpected movement of guidewire can cause fatal situation in coronary artery
            &lt;/li&gt;
            &lt;li&gt;
                Doctors must be able to monitor and intervene at emergency situation 
            &lt;/li&gt;
            &lt;li&gt;
                Intervention of doctor should have short latency
            &lt;/li&gt;
        &lt;/ul&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;container&quot;&gt; 
    &lt;div class=&quot;image&quot;&gt;
        &lt;img src=&quot;/images/medipixel/profile_biorobot_robotics.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;text&quot;&gt;
        &lt;h6&gt;Nonlinearity of manipulation&lt;/h6&gt;
        &lt;br /&gt;
        &lt;ul&gt;
            &lt;li&gt;
                Friction and twist of wire triggers difference between control input and output
            &lt;/li&gt;
            &lt;li&gt;
                Manipulation error in real environment (e.g. wire slip) was another issue in terms of error management
            &lt;/li&gt;
            &lt;li&gt;
                Control of flexible body has been a traditional hard problem 
            &lt;/li&gt;
        &lt;/ul&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;container&quot;&gt; 
    &lt;div class=&quot;image&quot;&gt;
        &lt;img src=&quot;/images/medipixel/profile_biorobot_cag.gif&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;text&quot;&gt;
        &lt;h6&gt;Complexity of environment&lt;/h6&gt;
        &lt;br /&gt;
        &lt;ul&gt;
            &lt;li&gt;
                Coronary arteries have complex physical structure including dynamic status changes like blood flow and heartbeat
            &lt;/li&gt;
            &lt;li&gt;
                It is possible that external factors (e.g. condition of patient) make abnormal environment transition  
            &lt;/li&gt;
        &lt;/ul&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h5 id=&quot;required-technologies&quot;&gt;Required Technologies&lt;/h5&gt;
&lt;p&gt;To undertake the complex project, my team prepared various technologies including medicine, robotics and reinforcement learning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To keep safety, we needed to comprehend medical knowledge for PCI&lt;/li&gt;
  &lt;li&gt;To achieve robust control, my team surveyed domains such as Robotics and Elastic Rod&lt;/li&gt;
  &lt;li&gt;To ascend accuracy of system, we studied Reinforcement Learning(RL) and Computer Vision Algorithm&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_tech.png&quot; width=&quot;95%&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;why-is-reinforcement-learning&quot;&gt;Why is Reinforcement Learning&lt;/h6&gt;
&lt;p&gt;Our environment was so complex that we did not have high confidence for traditional control method from robotics.
Since most of the traditional algorithms are static, it has shortages to handle dynamic environment.
Hence, to accomplish the task, we used “interaction algorithm”with the dynamic environment.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_mdp.gif&quot; width=&quot;63%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;main-tasks&quot;&gt;Main Tasks&lt;/h5&gt;
&lt;h6 id=&quot;planning&quot;&gt;Planning&lt;/h6&gt;
&lt;p&gt;To solve complex problem, we needed approaches step by step.
Through simplifying problem, we started it from the lowest dimension.
First, I set up project stages as follows&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;2D → 3D → 3D with vibration → Animal → Clinical environment&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h6 id=&quot;initiation&quot;&gt;Initiation&lt;/h6&gt;
&lt;p&gt;My role in this project was to decided direction of development direction as a development team leader.
I divided complicated main subject into several sub-tasks to materialize a plan.
I performed below tasks in each stage&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pre-research: To prepare knowledge for building of system&lt;/li&gt;
  &lt;li&gt;Set-up: To establish all experiments environment&lt;/li&gt;
  &lt;li&gt;Design: To design overall system architecture&lt;/li&gt;
  &lt;li&gt;Implementation: To implement the environment module in RL framework and integrate all modules&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;pre-research&quot;&gt;Pre-research&lt;/h5&gt;

&lt;h6 id=&quot;system-framework&quot;&gt;System Framework&lt;/h6&gt;
&lt;p&gt;I compared many different architectures for RL control system in real environment.
I researched existing systems published in conferences including &lt;a href=&quot;https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=8449910&quot;&gt;ICRA&lt;/a&gt;, &lt;a href=&quot;https://www.nema.org/pages/default.aspx&quot;&gt;NEMA&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/&quot;&gt;Arxiv&lt;/a&gt;. 
&lt;a href=&quot;https://arxiv.org/abs/1803.07067v1&quot;&gt;Setting up a Reinforcement Learning Task with a Real-World Robot&lt;/a&gt; was one of the most helpful experiment result in terms of understanding 1) relationship between System delays and System performance, 2) training results depending on action space, and 3) hierarchy of system in the real world&lt;/p&gt;

&lt;h6 id=&quot;medical-knowledge&quot;&gt;Medical Knowledge&lt;/h6&gt;
&lt;p&gt;We conducted interviews with doctors and researchers who have dealt with coronary artery disease. 
Also, we observed PCI procedure several times and studied on coronary arteries. 
In this process, we obtained knowledge as 1) procedure of PCI, 2) case studies by patient, 3) usage of equipments, 4) terms/abbreviation and 5) Procedure time&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_procedure.png&quot; width=&quot;95%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;set-up&quot;&gt;Set-up&lt;/h5&gt;

&lt;h6 id=&quot;establishment-of-experiment-plan&quot;&gt;Establishment of experiment plan&lt;/h6&gt;
&lt;p&gt;First environment was two dimensional blood vessel model.
I selected equipments for the system and drew a rough sketch of the experiment environment. 
Final expected environment design output is as below.
    &lt;figure&gt;
      &lt;img src=&quot;/images/medipixel/profile_biorobot_2d3denv.png&quot; width=&quot;90%&quot; /&gt;
    &lt;/figure&gt;&lt;/p&gt;

&lt;h6 id=&quot;installation-of-darkroom&quot;&gt;Installation of darkroom&lt;/h6&gt;
&lt;p&gt;Vision was the most important input data process for this system.
Since vision is sensitive to the change of illumination problem, I had to exclude natural light from experiment environment and darkroom was the best option to handle this problem. 
For the task, I was in charge of purchasing and installing all equipments for the darkroom. 
    &lt;figure&gt;
      &lt;img src=&quot;/images/medipixel/profile_biorobot_experiment_env.png&quot; width=&quot;90%&quot; /&gt;
    &lt;/figure&gt;&lt;/p&gt;

&lt;h6 id=&quot;comparison-of-cameras-by-latency&quot;&gt;Comparison of cameras by latency&lt;/h6&gt;
&lt;p&gt;Latency is one of the most significant factors for system performance. 
As a huge proportion of latency depended on camera, I selected a camera model carefully. 
As seen in the figure below, I conducted latency tests and compared scalability, compatibility, resolution and latency of varied camera model.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_realsense_test.gif&quot; width=&quot;65%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;design-principles&quot;&gt;Design Principles&lt;/h5&gt;

&lt;h6 id=&quot;modularity&quot;&gt;Modularity&lt;/h6&gt;
&lt;p&gt;Since this system is capable of having diverse environmental conditions like manipulator and external sensor devices, 
I looked for a way to minimize the number of additional tasks when subsystems or peripherals were changed, and suggested a form on separating the system into submodules by role and made hierarchy among them.&lt;/p&gt;

&lt;h6 id=&quot;scalability&quot;&gt;Scalability&lt;/h6&gt;
&lt;p&gt;As mentioned above, we planned an environment transition step by step. 
Thus, I had to enable smooth conversion among heterogeneous environments such as 2D, 3D, animal and clinic. 
Also, as we needed repetitive experiments for improving system performance like reward shaping, various settings for experiments had to be managed conveniently.
I achieved this purpose via abstract and inheritance structure.&lt;/p&gt;

&lt;h6 id=&quot;compatibility&quot;&gt;Compatibility&lt;/h6&gt;
&lt;p&gt;RL algorithm is necessary to be verified based on unit test. 
We used &lt;a href=&quot;https://gym.openai.com/envs/#atari&quot;&gt;Atari gym&lt;/a&gt; environment for test. 
I also considered standard communication protocol connecting with heterogeneously external devices.
For this reason, I designed this system by using de facto standard systems such as &lt;a href=&quot;https://gym.openai.com/&quot;&gt;openai-gym&lt;/a&gt; and &lt;a href=&quot;http://www.ros.org/&quot;&gt;ROS&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_arch.png&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;implementation-issues&quot;&gt;Implementation Issues&lt;/h5&gt;

&lt;h6 id=&quot;handling-nonlinearity&quot;&gt;Handling nonlinearity&lt;/h6&gt;
&lt;p&gt;There were physical errors during manipulation by motor rotation as many general controlling systems under physical world. 
In coronary artery environment, this kind of errors especially had a worse effect because it requires to handle exquisite unit of space and time. 
I approached this problem in a heuristic way and tried to define error tolerance thresholds because there is no perfect solution for this issue. 
In trial and error process, my team found that using a very small fixed step command(about 0.05mm) guaranteed that guidewire would be less affected by this problem and able to reach a correct position.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_slip.png&quot; width=&quot;90%&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;elaborate-data-flow-of-inter-module-communication&quot;&gt;Elaborate data flow of inter-module communication&lt;/h6&gt;
&lt;p&gt;To set proper shape and size of data, there were several trial and error. 
As shape of data required in each module was different, I pondered on computation cost of reshaping data while a current module was transferring data to next module. 
Also, because RL agent utilized experience replay, limitation of memory size for replay buffer was a big issue. 
Thus, size of state in RL had to be defined properly.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_data.png&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;synchronization-between-rl-agent-and-manipulator&quot;&gt;Synchronization between RL agent and manipulator&lt;/h6&gt;
&lt;p&gt;RL agent needs to obtain necessary data at once for decision making in each time step. 
But as a manipulator was operated in asynchronous method, I decided what module should be-waited and collected data from the manipulator for synchronization. 
I implemented communication module and put this module in charge of that task.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_sync.png&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;reduction-of-system-latency&quot;&gt;Reduction of system latency&lt;/h6&gt;
&lt;p&gt;Reactivity of system is one of the most critical factors in overall system performance since agile situation awareness and countermeasure were essential in PCI procedure. 
Thus, it was compulsory to minimize latency on each module because summation of delayed time took a huge proportion of reactivity. 
Especially, total latency largely depends on acquisition time of camera image and vision preprocessing time.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_latency.png&quot; width=&quot;95%&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;strict-exception-handling&quot;&gt;Strict exception handling&lt;/h6&gt;
&lt;p&gt;It was essential to handle and recover errors that cause harmful results strictly because this system was trained in real environment. 
I handled many abnormal situations like twisted guidewire and path deviation by excessive manipulation. 
Communication manipulator exception was another serious handling point because it could lead system procedure to be halted.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_twist.gif&quot; width=&quot;60%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;experiment&quot;&gt;Experiment&lt;/h5&gt;

&lt;h6 id=&quot;rl-algorithms&quot;&gt;RL algorithms&lt;/h6&gt;
&lt;p&gt;My team implemented and numerous experiments with RL algorithms to improve system performance.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Value based algorithms (&lt;a href=&quot;https://arxiv.org/abs/1710.02298&quot;&gt;Rainbow dqn&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1707.06887&quot;&gt;C51&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1806.06923&quot;&gt;IQN&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Demonstration algorithms (&lt;a href=&quot;https://arxiv.org/abs/1704.03732&quot;&gt;Deep Q-learning from Demonstrations&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Reward Shaping&lt;/li&gt;
  &lt;li&gt;Data fusion Execution Timing (Early fusion, Late fusion)&lt;/li&gt;
  &lt;li&gt;Additional (&lt;a href=&quot;https://arxiv.org/abs/1707.01495&quot;&gt;Hindsight Experience Replay&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;setup&quot;&gt;Setup&lt;/h6&gt;
&lt;p&gt;In navigation guidewire problem, selection of correct vessel branch is a main issue.
Thus, we focused on verifying it and designing experiment process.
Setting was below&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Max step is 700&lt;/li&gt;
  &lt;li&gt;Rewards are imposed by operating time and path correctness&lt;/li&gt;
  &lt;li&gt;Success of episode is to reach the target point within the max step&lt;/li&gt;
  &lt;li&gt;Goal positions are as below figure&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_goals.png&quot; width=&quot;60%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h5&gt;
&lt;p&gt;We made a successful result that guidewire reached basic goals in 2D blood vessel. 
Our navigation system succeeded about 95% to reach goal position.
Currently, my team and  researchers in AMC are writing a research paper targeting &lt;a href=&quot;https://www.crf.org/tct&quot;&gt;TCT&lt;/a&gt;, top tier medical conference.&lt;/p&gt;

&lt;h6 id=&quot;success-rate&quot;&gt;Success rate&lt;/h6&gt;
&lt;p&gt;The more training is carried out, the nearer success rate becomes to 1.0.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_successrate.png&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;time-spent&quot;&gt;Time spent&lt;/h6&gt;
&lt;p&gt;In early stage training, a wire moves forward regardless of success.
Thus, episode time spent on early stage is short.
However, after evolving period(400~600 episode step), system has high success rate and takes short time similar to time spent on the early stage.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_timespent.png&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;further&quot;&gt;Further&lt;/h5&gt;

&lt;h6 id=&quot;possibility-to-transfer-other-domain&quot;&gt;Possibility to transfer other domain&lt;/h6&gt;

&lt;p&gt;Because the skill-set includes controlling flexible medium and setting RL environment for those control system, we can utilize it to other fields.
We are having some meetings with persons from other domain and finding chances to apply our system to their fields.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Search system&lt;/th&gt;
      &lt;th&gt;Pipeline integrity inspection&lt;/th&gt;
      &lt;th&gt;Catheter procedure automation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/images/medipixel/profile_biorobot_newdomain0.png&quot; /&gt;Search system used to locate people in a collapsed building by manipulating wire camera&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/medipixel/profile_biorobot_newdomain1.png&quot; /&gt;Pipeline integrity inspection in a construction site&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/medipixel/profile_biorobot_newdomain2.png&quot; /&gt;Automation of other procedure through wire and catheter&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h6 id=&quot;next-plan&quot;&gt;Next Plan&lt;/h6&gt;
&lt;p&gt;We expect to advance for a new experiment project on 3D environment in the second half of 2019.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/profile_biorobot_3denv.png&quot; width=&quot;60%&quot; /&gt;
&lt;/figure&gt;</content><author><name>Hyungkyu Kim</name></author><summary type="html">The system places stent to the target lesion in the coronary artery via controlling manipulator made by Asan Medical Center. It is the world first autonomous PCI robot system. My team has developed reinforcement learning-based control software. This project was awarded first place at Johnson &amp;amp; Johnson QuickFire Challenge.</summary></entry><entry><title type="html">Lung Cancer Diagnosis Clinical Decision Support System</title><link href="http://localhost:4000/portfolio/medipixel/lung_cancer_diagnosis_clinical_decision_support_system" rel="alternate" type="text/html" title="Lung Cancer Diagnosis Clinical Decision Support System" /><published>2019-04-13T00:00:00+09:00</published><updated>2019-04-13T00:00:00+09:00</updated><id>http://localhost:4000/portfolio/medipixel/Lung%20Cancer%20Diagnosis%20Clinical%20Decision%20Support%20System</id><content type="html" xml:base="http://localhost:4000/portfolio/medipixel/lung_cancer_diagnosis_clinical_decision_support_system">&lt;p&gt;The purpose of the system is to help doctors make a decision on diagnosis process of lung cancer. It supports diagnosis by deep learning based on detection and classification nodules.&lt;/p&gt;

&lt;!--break--&gt;

&lt;h4 id=&quot;my-role&quot;&gt;My Role&lt;/h4&gt;
&lt;p&gt;Project Leader&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Design a whole data pipeline&lt;/li&gt;
  &lt;li&gt;Implement &lt;a href=&quot;https://en.wikipedia.org/wiki/DICOM&quot;&gt;DICOM&lt;/a&gt;-based data preprocessing module&lt;/li&gt;
  &lt;li&gt;Design and implement 3D U-net based detection neural network&lt;/li&gt;
  &lt;li&gt;Design and implement 3D Resnet based classification neural network&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;project-detail&quot;&gt;Project Detail&lt;/h4&gt;

&lt;!-- Feel free to change the width and height to your desired video size. --&gt;

&lt;div class=&quot;embed-container&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/MP9-zlR1cYc&quot; width=&quot;700&quot; height=&quot;480&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Lung cancer is placed in a top position in cancer death rate. 
In 2015 total 17,399 people was killed by lung cancer in Korea. 
It is fatal in that it takes 27.9% in the total amount of cancer deaths. 
The reason for its high death rate is that efficient inspection process does not exist yet.&lt;/p&gt;

&lt;p&gt;In many cases, biopsy is taken to confirm whether a patient has lung cancer or not.
It, however, should be avoided as much as possible since it is an invasive inspection. 
Usually chest computed tomography (CT) is used for biopsy screening, so radiologist’s role of reading CT correctly is very important.&lt;/p&gt;

&lt;p&gt;In the light of sharing experience accumulated over time, decision support system based on AI has been widely adopted in various industrial fields. 
In medical domain, the need of quick and precise diagnosis decision is intensifying. 
Hence, this kind of system has been being researched by many scientists and Medipixel’s lung cancer project is designed to fulfill those needs. 
We co-worked with &lt;a href=&quot;http://eng.amc.seoul.kr/gb/lang/main.do&quot;&gt;Asan Medical Center&lt;/a&gt; and ended up developing Lung Cancer Diagnosis Clinical Decision Support System.&lt;/p&gt;

&lt;p&gt;It takes the following advantages&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Detect nodules that can be missed → increased diagnosis accuracy&lt;/li&gt;
  &lt;li&gt;Assist doctors to read chest ct scan at ease → increased diagnosis productivity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/medipixel/mp_chest_system.png&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;system-features&quot;&gt;System Features&lt;/h5&gt;
&lt;p&gt;This system features are below&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Support diagnosis process whether nodule is malignant or benign&lt;/li&gt;
  &lt;li&gt;Handle non-small cell lung cancer nodule in 10mm~30mm range&lt;/li&gt;
  &lt;li&gt;Show the number of nodules and nodule region of interests&lt;/li&gt;
  &lt;li&gt;Indicate estimated lung cancer risk percentage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/medipixel/mp_screen_shot.png&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;tasks&quot;&gt;Tasks&lt;/h5&gt;
&lt;p&gt;I was involved in this project as a project leader who is in charge of designing overall process and data flow.&lt;/p&gt;

&lt;p&gt;I decided to approach this project in practical view. 
Since most of processes in deep learning system are formulaic, I did not exert much effort to design a system structure. 
Making totally new model was excluded from selections as well. Instead, we put more time finding high performance model adopted in medical domain and improving it.&lt;/p&gt;

&lt;h5 id=&quot;dataset&quot;&gt;Dataset&lt;/h5&gt;
&lt;p&gt;It is one of the most important issues in deep learning system to gather and handle proper dataset.
We utilized several dataset of hospitals involved with &lt;a href=&quot;http://eng.amc.seoul.kr/gb/lang/main.do&quot;&gt;Asan Medical Center&lt;/a&gt; and open dataset like &lt;a href=&quot;https://biometry.nci.nih.gov/cdas/nlst/&quot;&gt;NLST&lt;/a&gt; to improve performance of the system. 
There were some issues with datasets as follow&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Different data format(especially open data-set)&lt;/li&gt;
  &lt;li&gt;Different data modality such as resolution of CT&lt;/li&gt;
  &lt;li&gt;Overwhelmingly numerous malignant nodules(more than benign nodules)&lt;/li&gt;
  &lt;li&gt;Difficulty on finding totally negative data which did not have any nodule&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;preprocess&quot;&gt;Preprocess&lt;/h6&gt;
&lt;p&gt;We had to preprocess heterogeneous data set through one pipeline.
Basically, it was necessary to reconstruct and normalize from 2d DICOM slices to 3d voxel data. 
Since this task was expected to take a long time, it had to be done in advance, and results must be saved in hard disk before proceed regular training process.
Main process was like below.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Perform 3D reconstruction with thickness of 1mm&lt;/li&gt;
  &lt;li&gt;Set Hounsfield Unit between -1200 and 600&lt;/li&gt;
  &lt;li&gt;Normalize each pixel value between 0 and 255&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/mp_net_preprocess0.png&quot; width=&quot;80%&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Rather than using the whole voxel data, it was divided into chunks of specific size due to GPU memory limitation. 
This process occurred in training and inference routine.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/mp_net_preprocess1.png&quot; width=&quot;68%&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;gathering-dataset&quot;&gt;Gathering Dataset&lt;/h6&gt;
&lt;p&gt;To improve performance, we needed to obtain more dataset from various source.
This process collecting proper data for model requires long time cost for doctors. 
Especially, data labeling is a big overload because it is generally handcrafted task to segment the region of nodule in pixel by pixel.&lt;/p&gt;

&lt;p&gt;we exert much efforts to help them.
Key was an efficiency and decrease  of time consuming. 
We had several meetings with doctors and proposed useful methods such as nodule segmentation support application and semi-auto segmentation for nodule data.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/mp_chest_data0.jpg&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;deep-learning-networks&quot;&gt;Deep Learning Networks&lt;/h5&gt;
&lt;p&gt;In research period I realized that ensemble strategy was efficient to improve whole system accuracy. 
So, I got many experiments using various models and tried to find optimal combination. 
As a result we decided to use three Deep Neural Network(DNN) networks in the system.&lt;/p&gt;

&lt;h6 id=&quot;detection-network&quot;&gt;Detection network&lt;/h6&gt;
&lt;p&gt;This network detects nodules. 
It is built based on 3d U-net. 
Detection network consists of encoding and decoding network. 
Nodule’s features are extracted in encoding network through 3D residual blocks.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/mp_net_detection-net.png&quot; width=&quot;70%&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;segmentation-network&quot;&gt;Segmentation network&lt;/h6&gt;
&lt;p&gt;This network performs segmentation of nodules in each slide based on the results from detection network. 
Its second role is to perform as a filter for false-positive regions.
Our segmentation-network is taking advantage of &lt;a href=&quot;https://arxiv.org/abs/1802.02611&quot;&gt;Deeplab-v3+&lt;/a&gt; network whose network structure is shown in figure below&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/mp_net_segmentation-net.png&quot; width=&quot;70%&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;classification-network&quot;&gt;Classification network&lt;/h6&gt;
&lt;p&gt;Classification network is responsible for determining the degree of malignancy of candidate nodules. 
Features of nodules are extracted through 3D residual network layer, and we came up with final result through the features.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/mp_net_classification-net.png&quot; width=&quot;70%&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;practical-technique&quot;&gt;Practical Technique&lt;/h5&gt;
&lt;p&gt;I also added a simple preprocess stage before segmentation network to get rid of nodules in outer body.
This stage was composed of a simple method that just checks the largest contour and eroded it.
It mapped candidate nodule region onto 2D slide and examined whether it belongs to inside of the body, and regions outside of the body were removed as false-positives.
This method had some advantages over other methods that perform complicated lung-segmentation as follows&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is much faster than conventional segmentation method via complex algorithm&lt;/li&gt;
  &lt;li&gt;It removes an ordinary lung-segmentation algorithm problem that recognizes nodule on the wall as outside the lung&lt;/li&gt;
  &lt;li&gt;It solves region segmentation error resulting from different modality such as different CT slide scanner&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/mp_net_trick.png&quot; width=&quot;70%&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;strengths-of-the-structure&quot;&gt;Strengths of The Structure&lt;/h5&gt;
&lt;p&gt;Generally, most systems in cancer diagnosis has similar structure. 
Among these systems, combinations with U-net based detector and classifier were popular ones. 
&lt;strong&gt;I embedded simple preprocess stage and segmentation network&lt;/strong&gt; in center of it. 
Network mapped 3d suspicious chunks to 2d ct slice, at the same time this stage that performed to segment area in suspicious chunks meant scrutinizing and filtering nodules. 
So, I was able to &lt;strong&gt;remove many false positive nodules&lt;/strong&gt; from results in first stage. 
From those features, the system got simple but powerful improvement.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/medipixel/mp_chest_models.png&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h5&gt;
&lt;p&gt;Through interviews, we found that time spent on reading chest CT generally takes about from five to twenty minutes.
Our system achieved satisfying results that reduce time to under one minute while maintaining over 90% accuracy.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/mp_chest_auc.png&quot; width=&quot;60%&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;TSNE result was like below
&lt;img src=&quot;/images/medipixel/mp_chest_tsne.png&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;patent&quot;&gt;Patent&lt;/h5&gt;
&lt;p&gt;[1] Pathological diagnosis method and apparatus based on machine learning KR, P2018-0173475&lt;/p&gt;</content><author><name>Hyungkyu Kim</name></author><summary type="html">The purpose of the system is to help doctors make a decision on diagnosis process of lung cancer. It supports diagnosis by deep learning based on detection and classification nodules.</summary></entry><entry><title type="html">NIPS 2018 - AI for Prosthetics Challenge</title><link href="http://localhost:4000/portfolio/medipixel/nips_2018_-_AI_for_prosthetics_challenge" rel="alternate" type="text/html" title="NIPS 2018 - AI for Prosthetics Challenge" /><published>2019-04-02T00:00:00+09:00</published><updated>2019-04-02T00:00:00+09:00</updated><id>http://localhost:4000/portfolio/medipixel/NIPS%202018%20-%20AI%20for%20Prosthetics%20Challenge</id><content type="html" xml:base="http://localhost:4000/portfolio/medipixel/nips_2018_-_AI_for_prosthetics_challenge">&lt;p&gt;My team participate in &lt;a href=&quot;https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge&quot;&gt;NIPS 2018 - AI for Prosthetics Challenge&lt;/a&gt; and took a 17th place over 401 teams. 
We were able to get valuable &lt;a href=&quot;https://medipixel.github.io/NIPS2018&quot;&gt;practical experiences&lt;/a&gt; about Reinforcement Learning and Biomechanics domains.&lt;/p&gt;

&lt;!--break--&gt;

&lt;h4 id=&quot;my-role&quot;&gt;My Role&lt;/h4&gt;
&lt;p&gt;System main engineer&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Implement distributed Reinforcement Learning system(&lt;a href=&quot;https://rise.cs.berkeley.edu/projects/ray/&quot;&gt;RAY&lt;/a&gt;) on Cloud system(&lt;a href=&quot;https://azure.microsoft.com/&quot;&gt;Azure&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Implement RL-agent in &lt;a href=&quot;https://rise.cs.berkeley.edu/projects/ray/&quot;&gt;RAY&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;project-detail&quot;&gt;Project Detail&lt;/h4&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/nips_final.png&quot; width=&quot;80%&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;AI for Prosthetics Challenge was a official competition in &lt;a href=&quot;https://nips.cc/Conferences/2018/CompetitionTrack&quot;&gt;NeurIPS 2018 Competition Track&lt;/a&gt;. 
It used &lt;a href=&quot;http://opensim.stanford.edu/&quot;&gt;Opensim&lt;/a&gt; as a environment. 
The topic of this competition was to control neuromusculoskeletal model of person with prosthetics in the right leg for moving forward at the given velocity.&lt;/p&gt;

&lt;p&gt;We tried many approaches to save time and overcome shortage of resources. 
First, we researched &lt;a href=&quot;http://osim-rl.stanford.edu/docs/nips2017/solutions/&quot;&gt;previous competition&lt;/a&gt; and gathered useful information from them. 
Second, since my team was just two members, we divided the work up between each member cleary. 
The areas that my team focused on were RL method study, Opensim simulator study and imitation learning study.&lt;/p&gt;

&lt;h5 id=&quot;achievements&quot;&gt;Achievements&lt;/h5&gt;
&lt;p&gt;Since it was the first massive and practical RL project that my team undertook, we had numerous failures. 
But, in such attempts &lt;strong&gt;it was possible to obtain a skill-set to handle practical RL problem.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Handle complex environment(hundreds state values, 20  action values)&lt;/li&gt;
  &lt;li&gt;Handle sparse reward problem in RL&lt;/li&gt;
  &lt;li&gt;Build distributed Reinforcement Learning system(&lt;a href=&quot;https://rise.cs.berkeley.edu/projects/ray/&quot;&gt;RAY&lt;/a&gt;) on Cloud system(&lt;a href=&quot;https://azure.microsoft.com/&quot;&gt;Azure&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Utilize imitation learning method&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;environment-analysis&quot;&gt;Environment analysis&lt;/h5&gt;
&lt;p&gt;To begin with this competition, I had to analyse unfamiliar environment.
As opensim simulation and walking/running gait sequence were far from main task that I had handled before, I put efforts to understand it.&lt;/p&gt;

&lt;h6 id=&quot;opensim-environment&quot;&gt;Opensim environment&lt;/h6&gt;
&lt;p&gt;First, I started to verify RL environment space.
In &lt;a href=&quot;https://www.endtoend.ai/blog/ai-for-prosthetics-2/&quot;&gt;action space&lt;/a&gt;, it was a complex space controlling each muscle one by one compared to other general control methods utilizing joints velocity and angle.
Also &lt;a href=&quot;http://osim-rl.stanford.edu/docs/nips2018/observation/&quot;&gt;observation space&lt;/a&gt; was relatively complicated, because it had hundreds of data spectated from musculoskeletal.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/stanfordnmbl/osim-rl/1679344e509e29bdcc2ee368ddf83e868d93bf61/demo/random.gif&quot; width=&quot;50%&quot; /&gt;
  &lt;figcaption&gt;from (http://osim-rl.stanford.edu/) &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;opensim-tools&quot;&gt;Opensim tools&lt;/h6&gt;
&lt;p&gt;To achieve high score, I had to use &lt;a href=&quot;https://simtk.org/projects/nmbl_running&quot;&gt;experimental datasets&lt;/a&gt; for walking/running gait sequence in Opensim community. 
I tried to figure out the pattern of joint movements in gait cycle at the provided dataset.
There were several Opensim applications to solve biomechanical problems through forward and Inverse problem.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;https://medipixel.github.io/img/opensim/opensim_01.png&quot; width=&quot;70%&quot; /&gt;
  &lt;figcaption&gt;OpenSim: Simulating musculoskeletal dynamics and neuromuscular control to study human and animal movement&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Furthermore, it was necessary to analyze tools in code level, since built-in application had a limitation and I needed to modify experiment dataset in various ways.
After comprehension, I made it possible to customize tools based on project needs.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;https://medipixel.github.io/img/imitation/reward_edit_motion.PNG&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;utilization-of-distributed-rl&quot;&gt;Utilization of distributed RL&lt;/h5&gt;
&lt;p&gt;It was one of the most critical problems that interaction with simulator environment needed huge time while training. 
To improve training performance, my team used distributed Reinforcement Learning system.
It was implemented via &lt;a href=&quot;https://rise.cs.berkeley.edu/projects/ray/&quot;&gt;RAY&lt;/a&gt; on &lt;a href=&quot;https://azure.microsoft.com/&quot;&gt;Azure&lt;/a&gt; cloud server. 
We operated four cloud servers, and each server instance had 72 high-speed cores.
And all of server was clustered via Ray.
In result, It was possible to achieve much faster convergence of RL training results.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/nips_system.png&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;imitation-learning&quot;&gt;Imitation Learning&lt;/h5&gt;
&lt;p&gt;Sparse reward was a big trouble. 
In this competition, There was no clue for reward of RL agent except for the running speed. 
I tried to solve this problem with reward shaping and imitation learning method.&lt;/p&gt;

&lt;h6 id=&quot;demonstration&quot;&gt;Demonstration&lt;/h6&gt;
&lt;p&gt;First of all, I created running demonstration from provided dataset because RL agent needed to refer to expert’s trajectories.
Having limited period of gait cycle in Experiment dataset, I needed to insert additional kinematic motion data. 
So, I analysed pattern of gait from other referenced experiments(eg. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/18822415&quot;&gt;Muscle contributions to support and progression over a range&lt;/a&gt;) and made omitted data.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://medipixel.github.io/img/opensim/opensim_30002_ik.gif&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://medipixel.github.io/img/opensim/opensim_run_demo0.gif&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h6 id=&quot;behavioral-cloning-from-observation&quot;&gt;Behavioral Cloning from Observation&lt;/h6&gt;
&lt;p&gt;It failed to obtain actions from Opensim application with provided kinematic data. 
I needed to use a new approach that utilized only kinematic data for training. 
&lt;a href=&quot;https://arxiv.org/abs/1805.01954&quot;&gt;Behavioral Cloning from Observation&lt;/a&gt;(BCO) was one of the best options because Ray had BC agent and it was possible to implement BCO by exploiting built-in BC agent.
BCO was a model-based RL method that used model to infer actions from kinematics.
Therefore, I implemented &lt;a href=&quot;https://github.com/HyungKyu-Kim/ray/tree/master/python/ray/rllib/agents/bco&quot;&gt;BCO agent in Ray&lt;/a&gt;.&lt;/p&gt;

&lt;h6 id=&quot;deepmimic&quot;&gt;Deepmimic&lt;/h6&gt;
&lt;p&gt;Unfortunately, BCO method did not take notable score. 
Complexity of Opensim environment was too high to obtain meaningful state transition data from not-well-trained agent interactions. 
So I used state of the art method of imitation learning called &lt;a href=&quot;https://arxiv.org/abs/1804.02717&quot;&gt;Deepmimic&lt;/a&gt;.
I implemented &lt;a href=&quot;&quot;&gt;reward function of Deepmimic in Opensim environment&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;http://bair.berkeley.edu/static/blog/stuntman/teaser.gif&quot; /&gt;
  &lt;figcaption&gt;from https://bair.berkeley.edu/blog/2018/04/10/virtual-stuntman/&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;experiment&quot;&gt;Experiment&lt;/h5&gt;
&lt;p&gt;As deepmimic had a huge parameter set, I had to have many parameter-tuning tasks to improve score until project deadline.
Examples of early phase training by various parameters are like below.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/medipixel/nips_graph.PNG&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;result&quot;&gt;Result&lt;/h5&gt;
&lt;p&gt;Unlike my expectation, even we used Deepmimic method, agent could not imitate movement of demonstrations perfectly.
But Deepmimic was so helpful that agent achieved higher score than heuristic reward shaping method.
The final results were like below&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://medipixel.github.io/img/imitation/reward_ars_demo.gif&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://medipixel.github.io/img/imitation/reward_final.gif&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Hyungkyu Kim</name></author><summary type="html">My team participate in NIPS 2018 - AI for Prosthetics Challenge and took a 17th place over 401 teams. We were able to get valuable practical experiences about Reinforcement Learning and Biomechanics domains.</summary></entry><entry><title type="html">IoT Camera</title><link href="http://localhost:4000/portfolio/samsung/iot_camera" rel="alternate" type="text/html" title="IoT Camera" /><published>2019-03-31T00:00:00+09:00</published><updated>2019-03-31T00:00:00+09:00</updated><id>http://localhost:4000/portfolio/samsung/IoT%20Camera</id><content type="html" xml:base="http://localhost:4000/portfolio/samsung/iot_camera">&lt;p&gt;This new type of product is a network camera working as IoT gateway. 
It gathers sensor data and provides various service, such as social protection service, fire monitoring service and missing child prevention service, by cooperating with adjacent IoT devices.&lt;/p&gt;

&lt;!--break--&gt;

&lt;h4 id=&quot;my-role&quot;&gt;My Role&lt;/h4&gt;
&lt;p&gt;Network camera main developer at task force team&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Implement Bluetooth communication module between network camera and devices&lt;/li&gt;
  &lt;li&gt;Implement communication module with IoT server(RESTful)&lt;/li&gt;
  &lt;li&gt;Implement serial communication module(rs-485)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;project-detail&quot;&gt;Project Detail&lt;/h4&gt;

&lt;p&gt;Smart city is a fast-growing tech domain nowadays, and it is hard to imagine the smart city without IoT.
In order to achieve goal of Smart City Services, it is essential to cowork with widespread infrastructural devices in the city.
Moreover, security field has been moving forward from follow-up service using video information to convergence with information of infrastructural devices.&lt;/p&gt;

&lt;p&gt;Network Camera is a device to process video data which is one of the biggest data. 
Hence, it requires higher computation and network capacity compared to other basic sensor devices.&lt;/p&gt;

&lt;h5 id=&quot;prove-of-concept&quot;&gt;Prove of Concept&lt;/h5&gt;
&lt;p&gt;I had focused on this feature and tried to implement a few Prove of Concept to verify possibility of service  to provide value to customers.&lt;/p&gt;

&lt;h6 id=&quot;first-poc&quot;&gt;First PoC&lt;/h6&gt;
&lt;p&gt;The first PoC was to control smart bulb via camera to test connection process between camera and external devices. 
The main purpose of this PoC was controlling external device via inner event of camera.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/profile_htw_iot0.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h6 id=&quot;second-poc&quot;&gt;Second PoC&lt;/h6&gt;
&lt;p&gt;The second was to control camera by numerical value from outer sensor devices.
Order of PoC1 was opposite to that of PoC2. 
But ready made camera was not able to get bluetooth signal. 
To resolve this problem, I implemented and used simple progam on Raspberry pi.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/profile_htw_iot1.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;meaning-of-poc&quot;&gt;Meaning of PoC&lt;/h5&gt;

&lt;p&gt;After a few PoC, I found it possible that Network Camera was able to work as a Edge Gateway.
As mentioned above, camera has sufficient capability to handle high capacity video data without assistance from upper stage devices,&lt;/p&gt;

&lt;p&gt;Also in Smart City, one of the important points is data fusion.
It can realize services, once regarded as impossible, through  combination of data with various sensors that monitor item such as temperature, moisture, motion detection and image. 
If camera is a gateway to manage edge devices, data fusion is feasible without intervention from the high performance serve&lt;/p&gt;

&lt;h5 id=&quot;iot-camera&quot;&gt;IoT Camera&lt;/h5&gt;

&lt;p&gt;In early 2017, I got a chance to apply this concept to an actual product.
I joined Task Force Team as a main developer for camera, which is a sizeable project to cover city area.
Requirements of the project included various services such as missing child prevention and  fire monitoring service.&lt;/p&gt;

&lt;p&gt;My team, partner, and client had numerous disscusions to realize this concept in real world, and we decided to use bluetooth sensor devices and camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/samsung/profile_htw_iot2.png&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We made achievemets as follows&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Obtain sensor data and video data simultaneously
    &lt;ul&gt;
      &lt;li&gt;Service using sensor data and video data at the same time is feasible&lt;br /&gt;
 (eg: If there is not any target’s motion in a specific spot, camera will turn to the specific spot.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Provide various services via just changing external devices supporting bluetooth
    &lt;ul&gt;
      &lt;li&gt;Social Protection Service: Beacon&lt;/li&gt;
      &lt;li&gt;Fire Monitoring Service: BLE temperature sensor&lt;/li&gt;
      &lt;li&gt;Capable of attaching new services easily by adding new BLE devices&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Enable network load balancing
    &lt;ul&gt;
      &lt;li&gt;It divides computing cost concentrating on server to edge gateway&lt;/li&gt;
      &lt;li&gt;Data fusion can be feasible in this device&lt;/li&gt;
      &lt;li&gt;It can filter sensor data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;related-news&quot;&gt;Related News&lt;/h4&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://www.pntbiz.co.kr/index.php/2018/03/16/iot-cctv-blog/&quot;&gt;Server Partner company’s blog article(korean)&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&quot;https://www.youtube.com/watch?v=3PWR8SXjsU0&quot;&gt;Server Partner company’s IoT cctv demo(korean)&lt;/a&gt;&lt;br /&gt;
[3] &lt;a href=&quot;https://www.youtube.com/watch?v=IxaOIL74fu8&quot;&gt;Server Partner company’s IoT cctv Exhibition(korean)&lt;/a&gt;&lt;br /&gt;
[4] &lt;a href=&quot;http://www.newsis.com/view/?id=NISX20180731_0000378567&quot;&gt;Newsis news(korean)&lt;/a&gt;&lt;/p&gt;</content><author><name>Hyungkyu Kim</name></author><summary type="html">This new type of product is a network camera working as IoT gateway. It gathers sensor data and provides various service, such as social protection service, fire monitoring service and missing child prevention service, by cooperating with adjacent IoT devices.</summary></entry><entry><title type="html">Video Surveillance as a Service</title><link href="http://localhost:4000/portfolio/samsung/video_surveillance_as_a_service" rel="alternate" type="text/html" title="Video Surveillance as a Service" /><published>2019-03-30T00:00:00+09:00</published><updated>2019-03-30T00:00:00+09:00</updated><id>http://localhost:4000/portfolio/samsung/Video%20Surveillance%20as%20a%20Service</id><content type="html" xml:base="http://localhost:4000/portfolio/samsung/video_surveillance_as_a_service">&lt;p&gt;This service integrates cloud system and network camera. 
It can provide necessary information from network camera in the local network to the outside network user while keeping security.&lt;/p&gt;

&lt;!--break--&gt;

&lt;h4 id=&quot;my-role&quot;&gt;My Role&lt;/h4&gt;
&lt;p&gt;Network camera main developer&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Implement module establishing tunneling between server and camera&lt;/li&gt;
  &lt;li&gt;Implement controller module between server and camera&lt;/li&gt;
  &lt;li&gt;Implement module streaming between server and camera&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;project-detail&quot;&gt;Project Detail&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/images/samsung/profile_htw_cloud.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see from a name, CCTV(Closed Circuit Television) is a device working on closed local network. 
In network camera age, this concept was maintained as past. 
But this tendency had met technology called Cloud Service and started to move forward from closed world to open world.
“Video Surveillance as a Service” was created from this background.
VSaaS has advantages compred with traditional servailance systems.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Provide efficiency and total cost of ownership&lt;/li&gt;
  &lt;li&gt;Provide flexiblity and scalability&lt;/li&gt;
  &lt;li&gt;Access to value-added services&lt;/li&gt;
  &lt;li&gt;Provide a wide array of products for hosted video&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;stratocast&quot;&gt;Stratocast&lt;/h5&gt;
&lt;p&gt;I was participated in this domain as device side developer in 2015, when VSaaS was started to provide commercial service.
I started to work this interesting domain from &lt;a href=&quot;https://www.genetec.com/&quot;&gt;Genetek&lt;/a&gt;’s &lt;a href=&quot;https://www.genetec.com/solutions/all-products/stratocast/overview&quot;&gt;“Stratocast project”&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/profile_htw_cloud1.png&quot; /&gt;
  &lt;figcaption&gt;Stratocast implemented in Samsung camera&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;vsaas-camera&quot;&gt;VSaaS Camera&lt;/h5&gt;
&lt;p&gt;After that project, I was involved with development of the various client system in Network Camera device.
As each Cloud System had a distinct method to communicate between camera and server, I tried to find &lt;strong&gt;flexible hierarchy&lt;/strong&gt; to cope with these different features. 
This point was about &lt;strong&gt;dealing with heterogeneous protocols&lt;/strong&gt; with out huge transform.
Main issues were below&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Different connection protocol like VPN, SSH and custom protocol(socket)&lt;/li&gt;
  &lt;li&gt;Different system structure like existence of delegate connection server and region connection method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also had to handle &lt;strong&gt;variation of the connection network types&lt;/strong&gt; like.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ethernet&lt;/li&gt;
  &lt;li&gt;LTE&lt;/li&gt;
  &lt;li&gt;wifi&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And &lt;strong&gt;exception handle&lt;/strong&gt; was a significant topic. It had issues like following.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Compose device rebooting scenario(eg. reconnect)&lt;/li&gt;
  &lt;li&gt;Set connection retry count via limitation of server’s resources&lt;/li&gt;
  &lt;li&gt;Control amount of sending data by network bandwith&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Structure of VSaaS camera was below figure. My development part is in red rectangle.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/profile_htw_cloud.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;This client provided service through connection with server like following order.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Register Camera: Official operator performs secure process like RSA key exchange and registers camera in server&lt;/li&gt;
  &lt;li&gt;Sending Data: Registered camera sends video and additional data to streaming server through established tunnel&lt;/li&gt;
  &lt;li&gt;Streaming Service: Authenticated user requests the video streaming matched with user&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;related-news&quot;&gt;Related News&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://securitytoday.com/articles/2016/10/26/hanwha-techwin-america-cameras-earn-genetec-stratocast-vsaas-cloud-integration-and-certification.aspx&quot;&gt;Hanwha Techwin America Cameras Earn Genetec Stratocast VSaaS Cloud Integration and Certification &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sdmmag.com/articles/93266-cameras-earn-cloud-integration-certification&quot;&gt;Cameras Earn Cloud Integration &amp;amp; Certification &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hyungkyu Kim</name></author><summary type="html">This service integrates cloud system and network camera. It can provide necessary information from network camera in the local network to the outside network user while keeping security.</summary></entry><entry><title type="html">Other Projects</title><link href="http://localhost:4000/portfolio/samsung/other_projects" rel="alternate" type="text/html" title="Other Projects" /><published>2018-03-30T00:00:00+09:00</published><updated>2018-03-30T00:00:00+09:00</updated><id>http://localhost:4000/portfolio/samsung/Others%20Projects</id><content type="html" xml:base="http://localhost:4000/portfolio/samsung/other_projects">&lt;p&gt;Brief descriptions of other projects in Samsung Techwin.&lt;/p&gt;

&lt;!--break--&gt;

&lt;h4 id=&quot;my-role&quot;&gt;My Role&lt;/h4&gt;
&lt;p&gt;Network camera main developer&lt;/p&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;project-detail&quot;&gt;Project Detail&lt;/h4&gt;

&lt;p&gt;I had worked as a Product Engineering team member several years. Tasks of Product Engineering team were to handle additional requirements on Samsung Techwin’s products from other branch corporations. 
I was able to get many experiences in differnt system, frameworks and devices.&lt;/p&gt;

&lt;h5 id=&quot;tb-eye-project&quot;&gt;TB-EYE project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/On-screen_display&quot;&gt;OSD&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Work on various type OSD&lt;/li&gt;
  &lt;li&gt;Implement image based charecter OSD module&lt;/li&gt;
  &lt;li&gt;Handle raw image format(eg. yuv420, yuv422)&lt;/li&gt;
  &lt;li&gt;Implement C# window application&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/tbeye_pjt.jpg&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;military-important-facilities-project&quot;&gt;Military Important facilities project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Implement communication module&lt;/li&gt;
  &lt;li&gt;Design communication protocol&lt;/li&gt;
  &lt;li&gt;Communication module receives cgi command from central management system&lt;/li&gt;
  &lt;li&gt;Communication module controls PTZ housing via RS-485&lt;/li&gt;
  &lt;li&gt;Communication module controls sensors and other external devices via RS-485&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/military_pjt.jpg&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;sber-bank-project&quot;&gt;SBER bank project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Design communication protocol&lt;/li&gt;
  &lt;li&gt;Implement communication module via RS-232&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/sber_pjt.jpg&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;transportation-projects&quot;&gt;Transportation projects&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Implement input jpeg logo type OSD function&lt;/li&gt;
  &lt;li&gt;Implement input multi lines OSD function&lt;/li&gt;
  &lt;li&gt;Implement changing OSD size function&lt;/li&gt;
  &lt;li&gt;Handle raw image format(eg. yuv420, yuv422)&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/Transportation_pjt.jpg&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;austria-oebb-project&quot;&gt;Austria OEBB project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Implement motion detection function by index&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/austria_pjt.jpg&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;german-bonn-project&quot;&gt;German Bonn project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Train braking system overheating monitoring&lt;/li&gt;
  &lt;li&gt;Implement request to check overheating alarm and area to the camera encoder via RS-485&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/samsung/bonn_pjt.jpg&quot; width=&quot;70%&quot; /&gt;
&lt;/figure&gt;

&lt;h5 id=&quot;ivideon-project&quot;&gt;IVIDEON project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Implement camera LED contorl module via RESTful API&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;buthan-dam-project&quot;&gt;Buthan Dam project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Implement &lt;a href=&quot;https://www.unifore.net/ip-video-surveillance/network-camera-basic-what-s-defog-technology.html&quot;&gt;defog&lt;/a&gt; function&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;noemis-project&quot;&gt;Noemis project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Improve PTZ control via serial communication&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;agricultural-bank-project&quot;&gt;Agricultural bank project&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Implement control module connected with bill counter via RS-485&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;add-on-board-poc&quot;&gt;Add on board POC&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Implment video fusion between camera and thermal camera on raspberry pi&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;chilsung-engineering-project&quot;&gt;Chilsung engineering project:&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Implement camera control routine via RS485, AUX&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hyungkyu Kim</name></author><summary type="html">Brief descriptions of other projects in Samsung Techwin.</summary></entry></feed>